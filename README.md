# SAMWISE
Official repository for the paper: "SAMWISE: Infusing wisdom in SAM2 for Text-Driven Video Segmentation". In this work we build upon the Segment-Anything 2 (SAM2) model and make it wiser, by empowering it with natural language understanding and explicit temporal modeling  at the feature extraction stage, without fine-tuning its weights, and without outsourcing modality interaction to external models. Our proposed method, SAMWISE, achieves state-of-the-art across various benchmarks, by adding a negligible overhead of just 4.2 M parameters.

[![arXiv](https://img.shields.io/badge/arXiv-Preprint-red)]() **Paper Coming Soon!**


ğŸš€ **Code and Trained Models Coming Soon!** ğŸš€

<p align="center">
  <img src="./assets/method.png">
    <br/><em>Our proposed SAMWISE.</em>
</p>

### SAMWISE in Action ğŸ‘€
Our approach integrates natural language knowledge and temporal cues for <b>streaming-based Referring Video Segmentation (RVOS)</b>. We mitigate tracking biasâ€”where the model may overlook an identifiable object while tracking anotherâ€”through a learnable mechanism. This enables efficient streaming processing, leveraging memory from previous frames to maintain context and ensure accurate object segmentation.

<p align="center">
  <img src="./assets/teaser.png">
    <br/><em> SAMWISE for streaming-based RVOS.</em>
</p>


SAMWISE (our model, not the hobbit) segments objects from The Lord of the Rings in zero-shotâ€”no extra training, just living up to its namesake! ğŸ§™â€â™‚ï¸âœ¨

![Local GIF](./assets/lotr.gif)

